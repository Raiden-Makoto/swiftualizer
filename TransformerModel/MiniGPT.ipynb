{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11154199,"sourceType":"datasetVersion","datasetId":6959321}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:47:46.966192Z","iopub.execute_input":"2025-03-27T05:47:46.966443Z","iopub.status.idle":"2025-03-27T05:47:48.538284Z","shell.execute_reply.started":"2025-03-27T05:47:46.966422Z","shell.execute_reply":"2025-03-27T05:47:48.537571Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/taylorswiftlyrics/taylor_swift_lyrics.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:47:48.539074Z","iopub.execute_input":"2025-03-27T05:47:48.539551Z","iopub.status.idle":"2025-03-27T05:47:48.542930Z","shell.execute_reply.started":"2025-03-27T05:47:48.539517Z","shell.execute_reply":"2025-03-27T05:47:48.542193Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import keras\nfrom keras import layers, ops, optimizers\nfrom keras.layers import TextVectorization","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:47:48.543913Z","iopub.execute_input":"2025-03-27T05:47:48.544233Z","iopub.status.idle":"2025-03-27T05:48:05.010438Z","shell.execute_reply.started":"2025-03-27T05:47:48.544203Z","shell.execute_reply":"2025-03-27T05:48:05.009806Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import string\nimport random\n\nimport tensorflow as tf\nimport tensorflow.data as tf_data\nimport tensorflow.strings as tf_strings\nimport tensorflow_text as tf_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:48:05.011231Z","iopub.execute_input":"2025-03-27T05:48:05.011801Z","iopub.status.idle":"2025-03-27T05:48:05.440268Z","shell.execute_reply.started":"2025-03-27T05:48:05.011775Z","shell.execute_reply":"2025-03-27T05:48:05.439361Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"gpus = tf.config.list_physical_devices('GPU')\nif len(gpus) > 1:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        \n        # Use both GPUs\n        tf.config.set_visible_devices(gpus[:2], 'GPU')\n    except RuntimeError as e:\n        print(e)\n        \nstrategy = tf.distribute.MirroredStrategy()\nprint('Number of devices: {}'.format(strategy.num_replicas_in_sync))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:48:05.441313Z","iopub.execute_input":"2025-03-27T05:48:05.441648Z","iopub.status.idle":"2025-03-27T05:48:06.833882Z","shell.execute_reply.started":"2025-03-27T05:48:05.441618Z","shell.execute_reply":"2025-03-27T05:48:06.833137Z"}},"outputs":[{"name":"stdout","text":"Number of devices: 2\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import sentencepiece as spm\nimport tempfile\nimport re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:48:06.835717Z","iopub.execute_input":"2025-03-27T05:48:06.835979Z","iopub.status.idle":"2025-03-27T05:48:06.876936Z","shell.execute_reply.started":"2025-03-27T05:48:06.835941Z","shell.execute_reply":"2025-03-27T05:48:06.876032Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def casual_attention_mask(batch_size, n_dest, n_src, dtype):\n    i = ops.arange(n_dest)[:, None]\n    j = ops.arange(n_src)\n    m = i >= j - n_src + n_dest\n    mask = ops.cast(m, dtype)\n    mask = ops.reshape(mask, [1, n_dest, n_src])\n    mult = ops.concatenate(\n        [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])], 0\n    )\n    return ops.tile(mask, mult)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:48:06.879933Z","iopub.execute_input":"2025-03-27T05:48:06.880165Z","iopub.status.idle":"2025-03-27T05:48:06.884766Z","shell.execute_reply.started":"2025-03-27T05:48:06.880134Z","shell.execute_reply":"2025-03-27T05:48:06.883810Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0):\n        super().__init__()\n        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n        self.ffn = keras.Sequential([\n            layers.Dense(ff_dim, activation=\"relu\"),\n            layers.Dense(embed_dim)\n        ])\n        self.l1 = layers.LayerNormalization(epsilon=1e-6)\n        self.l2 = layers.LayerNormalization(epsilon=1e-6)\n        self.drop1 = layers.Dropout(rate)\n        self.drop2 = layers.Dropout(rate)\n\n    def call(self, inputs):\n        input_shape = ops.shape(inputs)\n        batch_size, seq_len = input_shape[0], input_shape[1]\n        casual_mask = casual_attention_mask(batch_size, seq_len, seq_len, \"bool\")\n        attn_output = self.att(inputs, inputs, attention_mask=casual_mask)\n        attn_output = self.drop1(attn_output)\n        out1 = self.l1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.drop1(ffn_output)\n        return self.l2(out1 + ffn_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:48:06.885414Z","iopub.execute_input":"2025-03-27T05:48:06.885610Z","iopub.status.idle":"2025-03-27T05:48:06.908080Z","shell.execute_reply.started":"2025-03-27T05:48:06.885593Z","shell.execute_reply":"2025-03-27T05:48:06.907513Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, max_len, vocab_size, embed_dim):\n        super().__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim = embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=max_len, output_dim=embed_dim)\n\n    def call(self, inputs):\n        max_len = ops.shape(inputs)[-1]\n        positions = ops.arange(0, max_len, 1)\n        positions = self.pos_emb(positions)\n        inputs = self.token_emb(inputs)\n        return inputs + positions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:49:00.147842Z","iopub.execute_input":"2025-03-27T05:49:00.148135Z","iopub.status.idle":"2025-03-27T05:49:00.152855Z","shell.execute_reply.started":"2025-03-27T05:49:00.148113Z","shell.execute_reply":"2025-03-27T05:49:00.152076Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"vocab_size = 32000\nmax_len = 324 # Victoria Park\nembed_dim = 256\nnum_heads = 3\nff_dim = 256","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:49:01.242534Z","iopub.execute_input":"2025-03-27T05:49:01.242857Z","iopub.status.idle":"2025-03-27T05:49:01.246767Z","shell.execute_reply.started":"2025-03-27T05:49:01.242833Z","shell.execute_reply":"2025-03-27T05:49:01.245857Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def create_model():\n    inputs = layers.Input(shape=(None,), dtype=\"int32\")\n    embedding = TokenAndPositionEmbedding(max_len, vocab_size, embed_dim)\n    transformer = TransformerBlock(embed_dim, num_heads, ff_dim)\n    x = embedding(inputs)\n    x = transformer(x)\n    outputs = layers.Dense(vocab_size)(x)\n    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    model.compile(\n        optimizers.Adam(learning_rate=5e-3),\n        loss=[loss_fn, None]\n    )\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:49:07.557973Z","iopub.execute_input":"2025-03-27T05:49:07.558298Z","iopub.status.idle":"2025-03-27T05:49:07.563339Z","shell.execute_reply.started":"2025-03-27T05:49:07.558271Z","shell.execute_reply":"2025-03-27T05:49:07.562440Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/taylorswiftlyrics/taylor_swift_lyrics.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:48:25.060677Z","iopub.execute_input":"2025-03-27T05:48:25.060948Z","iopub.status.idle":"2025-03-27T05:48:25.095974Z","shell.execute_reply.started":"2025-03-27T05:48:25.060929Z","shell.execute_reply":"2025-03-27T05:48:25.094851Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:48:25.253058Z","iopub.execute_input":"2025-03-27T05:48:25.253298Z","iopub.status.idle":"2025-03-27T05:48:25.277091Z","shell.execute_reply.started":"2025-03-27T05:48:25.253280Z","shell.execute_reply":"2025-03-27T05:48:25.276267Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                                Song  \\\n0        \"Slut!\" (Taylor's Version) (From The Vault)   \n1                              22 (Taylor's Version)   \n2                                          Afterglow   \n3  All Too Well (10 Minute Version) (Taylor's Ver...   \n4                    All Too Well (Taylor's Version)   \n\n                                              Lyrics  \n0  [Verse 1]\\nFlamingo pink, Sunrise Boulevard\\nC...  \n1  [Verse 1]\\nIt feels like a perfect night\\nTo d...  \n2  [Verse 1]\\nI blew things out of proportion, no...  \n3  [Verse 1]\\nI walked through the door with you,...  \n4  [Verse 1]\\nI walked through the door with you,...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Song</th>\n      <th>Lyrics</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\"Slut!\" (Taylor's Version) (From The Vault)</td>\n      <td>[Verse 1]\\nFlamingo pink, Sunrise Boulevard\\nC...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>22 (Taylor's Version)</td>\n      <td>[Verse 1]\\nIt feels like a perfect night\\nTo d...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Afterglow</td>\n      <td>[Verse 1]\\nI blew things out of proportion, no...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>All Too Well (10 Minute Version) (Taylor's Ver...</td>\n      <td>[Verse 1]\\nI walked through the door with you,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>All Too Well (Taylor's Version)</td>\n      <td>[Verse 1]\\nI walked through the door with you,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"lyrics_list = df[\"Lyrics\"].dropna().tolist()\nrandom.shuffle(lyrics_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:48:25.406484Z","iopub.execute_input":"2025-03-27T05:48:25.406699Z","iopub.status.idle":"2025-03-27T05:48:25.410908Z","shell.execute_reply.started":"2025-03-27T05:48:25.406680Z","shell.execute_reply":"2025-03-27T05:48:25.409917Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"contraction_pattern = re.compile(r\"\\b(?:[A-Za-z]+(?:'ll|'ve|'d|'re|'m|'s|n't))\\b\")\ncontractions_in_lyrics = set()\nfor line in lyrics_list:\n    contractions_in_lyrics.update(contraction_pattern.findall(line))\ncontractions_in_lyrics = sorted(contractions_in_lyrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:48:25.495680Z","iopub.execute_input":"2025-03-27T05:48:25.496014Z","iopub.status.idle":"2025-03-27T05:48:25.525396Z","shell.execute_reply.started":"2025-03-27T05:48:25.495987Z","shell.execute_reply":"2025-03-27T05:48:25.524345Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"with tempfile.NamedTemporaryFile(delete=False, mode=\"w\", encoding=\"utf-8\") as temp_file:\n    temp_file_name = temp_file.name\n    for line in lyrics_list:\n        temp_file.write(line + \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:48:26.867587Z","iopub.execute_input":"2025-03-27T05:48:26.867876Z","iopub.status.idle":"2025-03-27T05:48:26.874972Z","shell.execute_reply.started":"2025-03-27T05:48:26.867855Z","shell.execute_reply":"2025-03-27T05:48:26.874153Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"spm.SentencePieceTrainer.train(\n    input=temp_file_name,  \n    model_prefix=\"lyrics_tokenizer\",  \n    vocab_size=3446,\n    character_coverage=1.0,\n    user_defined_symbols=contractions_in_lyrics\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:48:27.073307Z","iopub.execute_input":"2025-03-27T05:48:27.073627Z","iopub.status.idle":"2025-03-27T05:48:27.272946Z","shell.execute_reply.started":"2025-03-27T05:48:27.073603Z","shell.execute_reply":"2025-03-27T05:48:27.271996Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"spm_model = spm.SentencePieceProcessor(model_file='lyrics_tokenizer.model')\nVOCAB = [spm_model.id_to_piece(i) for i in range(spm_model.vocab_size())]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:48:27.274451Z","iopub.execute_input":"2025-03-27T05:48:27.274800Z","iopub.status.idle":"2025-03-27T05:48:27.302321Z","shell.execute_reply.started":"2025-03-27T05:48:27.274767Z","shell.execute_reply":"2025-03-27T05:48:27.301545Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"with open(\"lyrics_tokenizer.model\", \"rb\") as f:\n    model_data = f.read()\n\nvectorizer = tf_text.SentencepieceTokenizer(model=model_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:48:27.340677Z","iopub.execute_input":"2025-03-27T05:48:27.340924Z","iopub.status.idle":"2025-03-27T05:48:27.350359Z","shell.execute_reply.started":"2025-03-27T05:48:27.340894Z","shell.execute_reply":"2025-03-27T05:48:27.349482Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"test_text = tf.constant([\"You're on the phone with your girlfriend\"])\nsus_text = vectorizer.detokenize(vectorizer.tokenize(test_text))\n\n# Output the tokenized result\nprint(sus_text.numpy().tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:48:27.495033Z","iopub.execute_input":"2025-03-27T05:48:27.495343Z","iopub.status.idle":"2025-03-27T05:48:27.505906Z","shell.execute_reply.started":"2025-03-27T05:48:27.495318Z","shell.execute_reply":"2025-03-27T05:48:27.505036Z"}},"outputs":[{"name":"stdout","text":"[b\"You're on the phone with your girlfriend\"]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"def prepare_gpt_inputs(text):\n    text = tf_strings.as_string(text)\n    tokenized = vectorizer.tokenize(text).to_tensor(\n        default_value=3500, shape=[None, 768]\n    )\n    x = tokenized[:, :-1]\n    y = tokenized[:, 1:]\n    return x, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:51:00.884143Z","iopub.execute_input":"2025-03-27T05:51:00.884449Z","iopub.status.idle":"2025-03-27T05:51:00.888859Z","shell.execute_reply.started":"2025-03-27T05:51:00.884427Z","shell.execute_reply":"2025-03-27T05:51:00.887944Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"text_ds = tf.data.Dataset.from_tensor_slices(lyrics_list)\ntext_ds = text_ds.shuffle(buffer_size=256)\ntext_ds = text_ds.batch(128) # batch_size\ntext_ds = text_ds.map(prepare_gpt_inputs, num_parallel_calls=tf_data.AUTOTUNE)\ntext_ds = text_ds.prefetch(tf_data.AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:51:02.035331Z","iopub.execute_input":"2025-03-27T05:51:02.035656Z","iopub.status.idle":"2025-03-27T05:51:02.108236Z","shell.execute_reply.started":"2025-03-27T05:51:02.035632Z","shell.execute_reply":"2025-03-27T05:51:02.107332Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"class SwiftGPT(keras.callbacks.Callback):\n    def __init__(self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=5):\n        self.max_tokens = max_tokens\n        self.start_tokens = start_tokens\n        self.index_to_word = index_to_word\n        self.print_every = print_every\n        self.top_k = top_k\n\n    def sample_from(self, logits):\n        logits, indices = ops.top_k(logits, k=self.top_k, sorted=True)\n        indices = np.array(indices).astype(\"int32\")\n        preds = keras.activations.softmax(ops.expand_dims(logits, 0))[0]\n        preds = np.array(preds).astype(\"float32\")\n        return np.random.choice(indices, p=preds)\n\n    def detokenize(self, number):\n        return self.index_to_word[number]\n\n    def on_epoch_end(self, epoch, logs=None):\n        start_tokens = [_ for _ in self.start_tokens]\n        if (epoch + 1) % self.print_every != 0: return\n        num_tokens_generated, tokens_generated = 0, []\n        \n        while num_tokens_generated <= self.max_tokens:\n            pad_len = max_len - len(start_tokens)\n            sample_index = len(start_tokens) - 1\n            if pad_len < 0:\n                x = start_tokens[:maxlen]\n                sample_index = maxlen - 1\n            elif pad_len > 0: x = start_tokens + [0] * pad_len\n            else: x = start_tokens\n            x = np.array([x])\n            y, _ = self.model.predict(x, verbose=0)\n            sample_token = self.sample_from(y[0][sample_index])\n            tokens_generated.append(sample_token)\n            start_tokens.append(sample_token)\n            num_tokens_generated = len(tokens_generated)\n\n        stuff = \" \".join([self.detokenize(_) for _ in self.start_tokens + tokens_generated])\n        print(stuff)\n        return","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:51:02.634982Z","iopub.execute_input":"2025-03-27T05:51:02.635305Z","iopub.status.idle":"2025-03-27T05:51:02.643555Z","shell.execute_reply.started":"2025-03-27T05:51:02.635278Z","shell.execute_reply":"2025-03-27T05:51:02.642489Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Tokenizer the starting prompt\nword_to_index = {}\nfor index, word in enumerate(VOCAB):\n    word_to_index[word] = index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:51:04.170995Z","iopub.execute_input":"2025-03-27T05:51:04.171303Z","iopub.status.idle":"2025-03-27T05:51:04.176003Z","shell.execute_reply.started":"2025-03-27T05:51:04.171278Z","shell.execute_reply":"2025-03-27T05:51:04.175068Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"start_prompt = \"You're on the phone with your girl friend she's upset\"\nstart_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\nnum_tokens_generated = 50\ncallback = SwiftGPT(num_tokens_generated, start_tokens, VOCAB)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:51:06.765731Z","iopub.execute_input":"2025-03-27T05:51:06.766070Z","iopub.status.idle":"2025-03-27T05:51:06.770413Z","shell.execute_reply.started":"2025-03-27T05:51:06.766043Z","shell.execute_reply":"2025-03-27T05:51:06.769438Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"with strategy.scope():\n    MiniGPT1 = create_model()\n    MiniGPT1.fit(text_ds, verbose=2, epochs=20, callbacks=[callback])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:51:09.448048Z","iopub.execute_input":"2025-03-27T05:51:09.448320Z","iopub.status.idle":"2025-03-27T05:52:32.960130Z","shell.execute_reply.started":"2025-03-27T05:51:09.448300Z","shell.execute_reply":"2025-03-27T05:52:32.959471Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'transformer_block_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n1/1 - 5s - 5s/step - loss: 10.4322\nEpoch 2/20\n1/1 - 1s - 1s/step - loss: 9.4074\nEpoch 3/20\n1/1 - 1s - 1s/step - loss: 7.7502\nEpoch 4/20\n1/1 - 1s - 1s/step - loss: 6.6525\nEpoch 5/20\nYou're on the <s> with your <s> <s> she's <s> ▁I ▁it ▁me ) ▁the ) ▁my ▁( ▁my ) ▁( ▁( ▁ ▁ ▁on ▁ ) ▁you ▁ ) ▁( ▁the ▁ ▁on ▁it ▁ ▁ ▁me ▁( ▁ ▁( ▁ ▁( ) ▁my ▁it ▁the ▁ ▁my ▁you ▁I ▁me ▁the ▁on ▁on ) ▁ ▁I ▁( ) ▁my\n1/1 - 14s - 14s/step - loss: 5.6831\nEpoch 6/20\n1/1 - 2s - 2s/step - loss: 5.2002\nEpoch 7/20\n1/1 - 1s - 1s/step - loss: 4.6741\nEpoch 8/20\n1/1 - 1s - 1s/step - loss: 4.5158\nEpoch 9/20\n1/1 - 1s - 1s/step - loss: 4.4536\nEpoch 10/20\nYou're on the <s> with your <s> <s> she's <s> ▁you ▁ , ▁to ▁you ▁you - ] ▁I ▁the ▁I ▁I ▁to ] ▁to ▁I ▁the ▁ ▁ ▁I ] ▁ ▁ Chorus , , , ▁[ ▁the ▁I ▁I ▁you ] , ▁I Chorus , ▁I ▁I ▁you , ▁I , ▁you ▁ ] ▁I ▁ Chorus ▁the ,\n1/1 - 13s - 13s/step - loss: 4.4814\nEpoch 11/20\n1/1 - 2s - 2s/step - loss: 4.2899\nEpoch 12/20\n1/1 - 1s - 1s/step - loss: 4.4265\nEpoch 13/20\n1/1 - 1s - 1s/step - loss: 4.3509\nEpoch 14/20\n1/1 - 1s - 1s/step - loss: 4.3924\nEpoch 15/20\nYou're on the <s> with your <s> <s> she's <s> ▁[ ▁to , ▁I ) ▁ ▁ ▁me ▁I ▁ ▁ ▁the , ▁you ) ▁ ) ) ▁[ ▁the ▁ ▁to ▁ , ▁ ▁to ▁me ▁ ▁the ▁you ▁I ▁ ▁to ▁I ▁ , ▁ , ▁the ) ▁[ , , , ) ▁the ▁ ▁ ▁a ▁me ▁me\n1/1 - 14s - 14s/step - loss: 4.4954\nEpoch 16/20\n1/1 - 2s - 2s/step - loss: 4.4381\nEpoch 17/20\n1/1 - 1s - 1s/step - loss: 4.3771\nEpoch 18/20\n1/1 - 1s - 1s/step - loss: 4.3668\nEpoch 19/20\n1/1 - 1s - 1s/step - loss: 4.1958\nEpoch 20/20\nYou're on the <s> with your <s> <s> she's <s> - ▁you ▁the ▁I ▁I ] ▁ ▁ I'm ▁[ ] ▁you ▁the ▁[ , , ▁a , ▁the - ▁I ] ▁[ ▁ , ▁ I'm , ▁the ] ▁ don't , - Chorus ▁[ ▁the ▁ - ▁you ] ▁you ▁a ▁you ▁a ▁to ▁the ▁you ▁the - ,\n1/1 - 14s - 14s/step - loss: 4.3780\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}