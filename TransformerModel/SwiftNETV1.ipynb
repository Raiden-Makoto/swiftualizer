{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "S4UF8T3jH0_I",
        "TgRHOTTfDKV4",
        "d4cjrI6vZKSA"
      ],
      "authorship_tag": "ABX9TyPola4IW6dbM+pFsPzLPjlK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raiden-Makoto/swiftualizer/blob/main/TransformerModel/SwiftNETV1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SwiftNET:\n",
        "**Keras + Tensorflow implementation of WaveNET specifically trained to generate pop songs in the style of Taylor Swift**"
      ],
      "metadata": {
        "id": "vs5PxgsfM0HB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0PcEGGOHR1S"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.data as tf_data\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "from keras.initializers import GlorotUniform, Zeros"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Operation for the main **SwiftNET** Model\n",
        "Code is sourced from [here](https://github.com/kokeshing/WaveNet-tf2/blob/master/model/module.py) but modified to use `Keras 3`."
      ],
      "metadata": {
        "id": "S4UF8T3jH0_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A causal convolution layer is a type of 1D convolution designed for sequential data where the output at time step $t$ depends only on inputs from time steps\n",
        "$ \\leq t$. This structure ensures that there is no \"leakage\" of future information into the past, making it useful for autoregressive models like WaveNet and time-series forecasting."
      ],
      "metadata": {
        "id": "SflQlPr9KctJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalConvolutionLayer(layers.Conv1D):\n",
        "    def __init__(\n",
        "            self,\n",
        "            filters,\n",
        "            kernel_size,\n",
        "            strides=1,\n",
        "            padding='causal',\n",
        "            dilation_rate=1,\n",
        "            residual_channels=None,\n",
        "            *args,\n",
        "            **kwargs\n",
        "        ):\n",
        "        super().__init__(\n",
        "            filters,\n",
        "            kernel_size,\n",
        "            strides=strides,\n",
        "            padding=padding,\n",
        "            dilation_rate=dilation_rate\n",
        "        )\n",
        "        self.k = kernel_size\n",
        "        self.d = dilation_rate\n",
        "        if kernel_size > 1:\n",
        "            self.queue_len = kernel_size + (kernel_size - 1) * (dilation_rate - 1)\n",
        "            self.queue_dim = residual_channels\n",
        "            self.init_queue()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super().build(input_shape)\n",
        "        self.linearized_weights = ops.cast(\n",
        "            ops.reshape(self.kernel, [-1, self.filters]),\n",
        "            dtype=tf.float32\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, is_synthesis=False):\n",
        "        if not is_synthesis: return super().call(inputs)\n",
        "        if self.k > 1:\n",
        "            self.queue = self.queue[:, 1:, :]\n",
        "            self.queue = ops.concatenate(\n",
        "                [self.queue, ops.expand_dims(inputs[:, -1, :], axis=1)],\n",
        "                axis=1\n",
        "            )\n",
        "            if self.d > 1: inputs = self.queue[:, 0::self.d, :]\n",
        "            else: inputs = self.queue\n",
        "        outputs = ops.matmul(ops.reshape(inputs, [1, -1]), self.linearized_weights)\n",
        "        outputs = keras.backend.bias_add(outputs, self.bias)\n",
        "        return tf.reshape(outputs, [-1, 1, self.filters])\n",
        "\n",
        "    def init_queue(self):\n",
        "        self.queue = ops.zeros([1, self.queue_len, self.queue_dim], dtype=tf.float32)"
      ],
      "metadata": {
        "id": "CRb6GmCeHiwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConv1DGLU(keras.Model):\n",
        "    \"\"\"conv1d + GLU => add condition => residual add + skip connection\"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            residual_channels,\n",
        "            gate_channels,\n",
        "            kernel_size,\n",
        "            skip_out_channels=None,\n",
        "            dilation_rate=1,\n",
        "            **kwargs\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.residual_channels = residual_channels\n",
        "        if skip_out_channels is None: skip_out_channels = residual_channels\n",
        "\n",
        "        self.dilated_conv = CausalConvolutionLayer(\n",
        "            gate_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            padding='causal',\n",
        "            dilation_rate=dilation_rate,\n",
        "            residual_channels=residual_channels\n",
        "        )\n",
        "        self.conv_c = CausalConvolutionLayer(\n",
        "            gate_channels,\n",
        "            kernel_size=1,\n",
        "            padding='causal'\n",
        "        )\n",
        "        self.conv_skip = CausalConvolutionLayer(\n",
        "            skip_out_channels,\n",
        "            kernel_size=1,\n",
        "            padding='causal'\n",
        "        )\n",
        "        self.conv_out = CausalConvolutionLayer(\n",
        "            residual_channels,\n",
        "            kernel_size=1,\n",
        "            padding='causal'\n",
        "        )\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs, c):\n",
        "        x = self.dilated_conv(inputs)\n",
        "        x_tanh, x_sigmoid = ops.split(x, indices_or_sections=2, axis=2)\n",
        "        c = self.conv_c(c)\n",
        "        c_tanh, c_sigmoid = tf.split(c, indices_or_sections=2, axis=2)\n",
        "\n",
        "        x_tanh, x_sigmoid = x_tanh + c_tanh, x_sigmoid + c_sigmoid\n",
        "        x = tf.nn.tanh(x_tanh) * tf.nn.sigmoid(x_sigmoid)\n",
        "\n",
        "        s = self.conv_skip(x)\n",
        "        x = self.conv_out(x)\n",
        "\n",
        "        x = x + inputs\n",
        "\n",
        "        return x, s\n",
        "\n",
        "    def init_queue(self):\n",
        "        self.dilated_conv.init_queue()\n",
        "\n",
        "    def synthesis_feed(self, inputs, c):\n",
        "        x = self.dilated_conv(inputs, is_synthesis=True)\n",
        "        x_tanh, x_sigmoid = tf.split(x, num_or_size_splits=2, axis=2)\n",
        "\n",
        "        c = self.conv_c(c, is_synthesis=True)\n",
        "        c_tanh, c_sigmoid = tf.split(c, num_or_size_splits=2, axis=2)\n",
        "\n",
        "        x_tanh, x_sigmoid = x_tanh + c_tanh, x_sigmoid + c_sigmoid\n",
        "        x = tf.nn.tanh(x_tanh) *keras.activations.sigmoid(x_sigmoid)\n",
        "        s = self.conv_skip(x, is_synthesis=True)\n",
        "        x = self.conv_out(x, is_synthesis=True)\n",
        "        x = x + inputs\n",
        "        return x, s"
      ],
      "metadata": {
        "id": "q8VO2HqdBQZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create UpSampling Layers\n",
        "In WaveNet, upsampling layers are be applied at the final output layer to expand or refine the predictions (e.g., generating high-frequency audio details)."
      ],
      "metadata": {
        "id": "TgRHOTTfDKV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UpsampleConv(tf.keras.Model):\n",
        "    def __init__(self, rate, **kwargs):\n",
        "        super().__init__()\n",
        "        self.upsampling = layers.UpSampling2D(\n",
        "            size=(1, rate),\n",
        "            interpolation='nearest'\n",
        "        )\n",
        "        self.conv = layers.Conv2D(\n",
        "            filters=1,\n",
        "            kernel_size=(1, rate * 2 + 1),\n",
        "            padding='same',\n",
        "            use_bias=False,\n",
        "            kernel_initializer=tf.constant_initializer(1. / (rate * 2 + 1))\n",
        "        )\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        return self.conv(self.upsampling(x))"
      ],
      "metadata": {
        "id": "6LrHtTTnDnMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpsampleNetwork(tf.keras.Model):\n",
        "    def __init__(self, upsample_scales, **kwargs):\n",
        "        super().__init__()\n",
        "        self.upsample_layers = [UpsampleConv(scale) for scale in upsample_scales]\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, feat):\n",
        "        for layer in self.upsample_layers:\n",
        "            feat = layer(feat)\n",
        "        return feat"
      ],
      "metadata": {
        "id": "HAj2vV7LEfkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mu Law Quantization\n",
        "Companding (short for compressing + expanding) is a technique used in signal processing to reduce the dynamic range of a signal before quantization and restore it afterward.\n",
        "\n",
        "It helps to improve signal quality, reduce quantization noise, and optimize storage or transmission efficiencyâ€”especially in audio, speech processing, and telecommunications.\n",
        "\n",
        "The mu-law transformation is a nonlinear companding algorithm used in digital audio processing and speech compression. It reduces the dynamic range of an audio signal, improving quantization at lower amplitudes while preserving detail in louder signals."
      ],
      "metadata": {
        "id": "d4cjrI6vZKSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MuLawQuantize(x, quantization_channels=255):\n",
        "    x = ops.sign(x) * ops.log1p(quantization_channels * ops.abs(x)) / ops.log1p(quantization_channels)\n",
        "    return quantization_channels * ops.floor(0.5 + x)"
      ],
      "metadata": {
        "id": "O3U8O39k3_U7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def InverseMuLawQuantize(x, quantization_channels=255):\n",
        "    x = 2 * ops.cast(x, dtype=np.float32) / quantization_channels - 1\n",
        "    return ops.sign(x) * (1.0 / quantization_channels) * ((1.0 + quantization_channels) ** ops.abs(x) - 1.0)"
      ],
      "metadata": {
        "id": "tf8XwtFwTEOU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the SwiftNET Model"
      ],
      "metadata": {
        "id": "rQWv4zykN9Cq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SwiftNET(keras.Model):\n",
        "    def __init__(self, num_mel, upsample_scales, **kwargs):\n",
        "        super().__init__()\n",
        "        self.upsampler = UpsampleNetwork(upsample_scales)\n",
        "        self.initial = CausalConvolutionLayer(\n",
        "            filters = 128,\n",
        "            kernel_size = 1,\n",
        "            padding = 'causal'\n",
        "        )\n",
        "        self.residual_blocks = []\n",
        "        for _ in range(2):\n",
        "            for exponent in range(10):\n",
        "                self.residual_blocks.append(\n",
        "                    ResidualConv1DGLU(\n",
        "                        residual_channels = 128,\n",
        "                        gate_channels = 256,\n",
        "                        kernel_size = 3,\n",
        "                        skip_out_channels = 128,\n",
        "                        dilation_rate = 2 ** exponent\n",
        "                    )\n",
        "                )\n",
        "        self.postprocessing = [\n",
        "            layers.ReLU(),\n",
        "            layers.Conv1D(\n",
        "                filters = 128,\n",
        "                kernel_size = 1,\n",
        "                padding = 'causal'\n",
        "            ),\n",
        "            layers.ReLU(),\n",
        "            layers.Conv1D(\n",
        "                filters = 256,\n",
        "                kernel_size = 1,\n",
        "                padding = 'causal'\n",
        "            )\n",
        "        ]\n",
        "\n",
        "    def init_queue(self):\n",
        "        for block in self.residual_blocks:\n",
        "            block.init_queue()\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "        c = ops.expand_dims(c, axis=-1)\n",
        "        c = self.upsample_network(c)\n",
        "        c = ops.transpose(ops.squeeze(c, axis=-1), axes=(0, 2, 1))\n",
        "        x = self.initial(inputs)\n",
        "        skips = None\n",
        "        for block in self.residual_blocks:\n",
        "            x, h = block(x, c)\n",
        "            if skips is None: skips = h\n",
        "            else: skips = skips + h\n",
        "        x = skips\n",
        "        for layer in self.postprocessing:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "W-oBzK8CZFnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameters"
      ],
      "metadata": {
        "id": "vU0rwI2XNRm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "UPSAMPLE_SCALES = [3, 7, 21]\n",
        "\n",
        "SEQ_LEN = 44100\n",
        "SR = 44100\n",
        "NUM_MELS = 128\n",
        "N_FFT = 2048\n",
        "HOP_SIZE = 441\n",
        "WIN_SIZE = 2048\n",
        "\n",
        "LR = 1e-4\n",
        "DECAY_RATE = 0.8\n",
        "DECAY_STEPS = int(4e5)\n",
        "EPOCHS = 1000\n",
        "BATCH_SIZE = 9\n",
        "\n",
        "N_TEST_SAMPLE = 5\n",
        "SAVE_IVAL = 50"
      ],
      "metadata": {
        "id": "KvBVtknBbJ44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Dataset: Utilities\n",
        "`Librosa` is a Python library specifically designed to handle `.wav` files we will be working with"
      ],
      "metadata": {
        "id": "V30CuJ-EPewh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "from scipy.io import wavfile"
      ],
      "metadata": {
        "id": "zXnkjq9OO5kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def files_to_list(filename):\n",
        "    with open(filename, encoding=\"utf-8\") as f:\n",
        "        files = f.readlines()\n",
        "    files = [file.rstrip() for file in files]\n",
        "    return files"
      ],
      "metadata": {
        "id": "RuuOq6idQ-ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_wav(path, sampling_rate: int=SR):\n",
        "    \"\"\"Loads a .wav file. Uses the default sampling rate of 44100 for mono audio.\"\"\"\n",
        "    wav = librosa.load(path, sr=sampling_rate, mono=True)[0]\n",
        "    return wav"
      ],
      "metadata": {
        "id": "CKS8fdjQRK0U"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trim_silence(\n",
        "        wav,\n",
        "        top_db: float=40.0,\n",
        "        fft_size: int=2048,\n",
        "        hop_size: int=HOP_SIZE\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Trims the silence from an audio signal based on a threshold of loudness.\n",
        "\n",
        "    Args:\n",
        "        wav (ndarray): The input audio waveform (1D numpy array).\n",
        "        top_db (float): The threshold (in decibels) below reference to consider as silence.\n",
        "        fft_size (int): The size of the FFT window used in the trimming process.\n",
        "        hop_size (int): The hop size (or stride) used in the trimming process.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: The trimmed audio signal.\n",
        "    \"\"\"\n",
        "    trimmed_audio, _ =  librosa.effects.trim(\n",
        "        wav,\n",
        "        top_db=top_db,\n",
        "        frame_length=fft_size,\n",
        "        hop_length=hop_size\n",
        "    )\n",
        "    return trimmed_audio"
      ],
      "metadata": {
        "id": "tID9939rRhe5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalize = lambda wav: librosa.util.normalize(wav)"
      ],
      "metadata": {
        "id": "BnlxlbhwSOYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_wav(wav, path: str, sr: int=SR):\n",
        "    \"\"\"Saves a .wav file\"\"\"\n",
        "    wav *= 32767 / max(0.0001, np.max(np.abs(wav)))\n",
        "    wavfile.write(path, sr, wav.astype(np.int16))"
      ],
      "metadata": {
        "id": "HY0_1GytSrAl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MelSpectrogram(\n",
        "    wav,\n",
        "    sampling_rate: int = SR,\n",
        "    num_mels: int = NUM_MELS,\n",
        "    n_fft: int = N_FFT,\n",
        "    hop_size: int = HOP_SIZE,\n",
        "    win_size: int = WIN_SIZE\n",
        "):\n",
        "    d = librosa.stft(y=wav, n_fft=n_fft, hop_length=hop_size, win_length=win_size, pad_mode='constant')\n",
        "    mel_filter = librosa.filters.mel(sampling_rate, n_fft, n_mels=num_mels)\n",
        "    s = np.dot(mel_filter, np.abs(d))\n",
        "    return np.log10(np.maximum(s, 1e-5))"
      ],
      "metadata": {
        "id": "03okH-zAUcOo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9J935z6gU-xO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}